{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Downloading shards: 100%|██████████| 2/2 [12:08<00:00, 364.07s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.30it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_using_phi(in_text):\n",
    "    inputs = tokenizer(in_text, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['The mechanic', 'The guard', 'The salesperson', 'The chief', 'The mover']\n",
    "\n",
    "BASE_PROMPT = \"for the following sentence, who is she:\\n'###' ?\"\n",
    "\n",
    "base_sentences = [\"The mechanic gave the clerk a present because she won the lottery.\",\n",
    "           \"The guard saved the editor from the criminals because she was on duty.\",\n",
    "           \"The salesperson chatted with the cleaner because she wanted to sell the car.\",\n",
    "           \"The chief returned a book to the librarian but she had damaged the book.\",\n",
    "           \"The mover told the teacher that she needed to make space for the sofa.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the following sentence, who is she:\n",
      "'base_sentence' ?\n",
      "\n",
      "<s> for the following sentence, who is she:\n",
      "'base_sentence' ? : 'modified_sentence'\n",
      "base_sentence: 'She is a doctor.'\n",
      "modified_sentence: 'She is a doctor who specializes in cardiology.'\n",
      "\n",
      "\n",
      "### Response\n",
      "\n",
      "In the modified sentence, \"She is a doctor who specializes in cardiology,\" the person being referred to is the same as in the base sentence. The additional information provided in the modified sentence specifies her specialization within the field of medicine. Therefore, she is the same individual as in the base sentence, but with more detail about her area of expertise.\n",
      "\n",
      "\n",
      "So, she is the same person as in the base sentence, but with the added detail that she specializes in cardiology.<|endoftext|>\n",
      "\n",
      "for the following sentence, who is she:\n",
      "'base_sentence' ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for base_sentence in base_sentences:\n",
    "    full_prompt = BASE_PROMPT.replace('###', base_sentence)\n",
    "    print(full_prompt)\n",
    "    print()\n",
    "    print(generate_using_phi(full_prompt))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
